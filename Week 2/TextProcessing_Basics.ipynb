{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1. SETUP & DATA ---\n",
        "print(\"--- 0. Setup ---\")\n",
        "# Download necessary NLTK components (run once)\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    print(\"NLTK components downloaded successfully (punkt, stopwords, wordnet).\")\n",
        "except Exception as e:\n",
        "    print(f\"NLTK download failed. Ensure you have network access. Error: {e}\")\n",
        "\n",
        "# Example Corpus (used for TF-IDF)\n",
        "CORPUS = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A dog is lazy and a fox is quick.\",\n",
        "    \"Quick animals like foxes and dogs are often found in nature.\"\n",
        "]\n",
        "\n",
        "# Example Document (used for Tokenization, Normalization, Lemmatization demo)\n",
        "DOCUMENT_TO_PROCESS = \"The running foxes are quickly jumping over the dogs! Do they like running?\"\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 2. TOKENIZATION, NORMALIZATION, AND LEMMATIZATION ---\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Applies Tokenization, Normalization, and Lemmatization to a single piece of text.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[INPUT TEXT]: {text}\")\n",
        "\n",
        "    # 2a. TOKENIZATION\n",
        "    # Breaking the text into individual words or tokens\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"\\n--- 2a. TOKENIZATION ---\")\n",
        "    print(f\"Total tokens: {len(tokens)}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "\n",
        "    # 2b. NORMALIZATION (Lowercasing and removing punctuation/stop words)\n",
        "    print(\"\\n--- 2b. NORMALIZATION ---\")\n",
        "\n",
        "    # 2b.i Lowercasing\n",
        "    tokens_lower = [token.lower() for token in tokens]\n",
        "    print(f\"After Lowercasing: {tokens_lower}\")\n",
        "\n",
        "    # 2b.ii Removing punctuation\n",
        "    punctuation_list = string.punctuation\n",
        "    tokens_no_punct = [token for token in tokens_lower if token not in punctuation_list]\n",
        "    print(f\"After Punctuation Removal: {tokens_no_punct}\")\n",
        "\n",
        "    # 2b.iii Removing stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens_no_punct if token not in stop_words]\n",
        "    print(f\"After Stop Word Removal: {filtered_tokens}\")\n",
        "\n",
        "\n",
        "    # 2c. LEMMATIZATION\n",
        "    # Reducing words to their base or root form (e.g., 'running' -> 'run', 'dogs' -> 'dog')\n",
        "    print(\"\\n--- 2c. STEMMING/LEMMATIZATION (Using Lemmatization) ---\")\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [lemmatizer.lemmatize(token, pos='v') for token in filtered_tokens] # pos='v' for verbs\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in lemmas] # default pos='n' for nouns\n",
        "\n",
        "    print(f\"Final Lemmatized Tokens: {lemmas}\")\n",
        "\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "# Run the preprocessing on the example document\n",
        "print(\"--- 1. Preprocessing Demo (Tokenization, Normalization, Lemmatization) ---\")\n",
        "processed_document = preprocess_text(DOCUMENT_TO_PROCESS)\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 3. TF-IDF CALCULATION ---\n",
        "\n",
        "def calculate_tfidf(corpus):\n",
        "    \"\"\"\n",
        "    Calculates the Term Frequency-Inverse Document Frequency (TF-IDF) scores.\n",
        "    \"\"\"\n",
        "    print(\"--- 3. TF-IDF CALCULATION ---\")\n",
        "    print(\"\\n[CORPUS USED FOR TF-IDF]:\")\n",
        "    for i, doc in enumerate(corpus):\n",
        "        print(f\"  Document {i+1}: {doc}\")\n",
        "\n",
        "    # Initialize the TfidfVectorizer (this handles tokenization, normalization, and smoothing internally)\n",
        "    # We use our own preprocessing for the previous steps, but for a standard TF-IDF calculation,\n",
        "    # TfidfVectorizer is the most efficient method.\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Fit the model to the corpus and transform the corpus into a matrix of TF-IDF scores\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Get feature names (the vocabulary)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Convert the matrix to a dense array and then to a pandas DataFrame for clear viewing\n",
        "    tfidf_array = tfidf_matrix.toarray()\n",
        "    tfidf_df = pd.DataFrame(tfidf_array, columns=feature_names,\n",
        "                            index=[f'Document {i+1}' for i in range(len(corpus))])\n",
        "\n",
        "    # Print the resulting TF-IDF Matrix\n",
        "    print(\"\\n[TF-IDF MATRIX (Term Frequency-Inverse Document Frequency)]:\")\n",
        "    print(\"Scores indicate term importance relative to the entire corpus.\")\n",
        "    print(tfidf_df.round(4))\n",
        "\n",
        "    print(\"\\n[INTERPRETATION EXAMPLE]:\")\n",
        "    print(\"The word 'quick' has a high score in Document 1 and 2 but a lower score in Document 3,\")\n",
        "    print(\"meaning it is slightly less unique to Document 3 compared to the other two.\")\n",
        "\n",
        "# Run the TF-IDF calculation on the corpus\n",
        "calculate_tfidf(CORPUS)\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXaHSBPQ7uJd",
        "outputId": "5416ac24-9060-4d80-c46c-2bfb263eab0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0. Setup ---\n",
            "NLTK components downloaded successfully (punkt, stopwords, wordnet).\n",
            "--------------------------------------------------\n",
            "--- 1. Preprocessing Demo (Tokenization, Normalization, Lemmatization) ---\n",
            "\n",
            "[INPUT TEXT]: The running foxes are quickly jumping over the dogs! Do they like running?\n",
            "\n",
            "--- 2a. TOKENIZATION ---\n",
            "Total tokens: 15\n",
            "Tokens: ['The', 'running', 'foxes', 'are', 'quickly', 'jumping', 'over', 'the', 'dogs', '!', 'Do', 'they', 'like', 'running', '?']\n",
            "\n",
            "--- 2b. NORMALIZATION ---\n",
            "After Lowercasing: ['the', 'running', 'foxes', 'are', 'quickly', 'jumping', 'over', 'the', 'dogs', '!', 'do', 'they', 'like', 'running', '?']\n",
            "After Punctuation Removal: ['the', 'running', 'foxes', 'are', 'quickly', 'jumping', 'over', 'the', 'dogs', 'do', 'they', 'like', 'running']\n",
            "After Stop Word Removal: ['running', 'foxes', 'quickly', 'jumping', 'dogs', 'like', 'running']\n",
            "\n",
            "--- 2c. STEMMING/LEMMATIZATION (Using Lemmatization) ---\n",
            "Final Lemmatized Tokens: ['run', 'fox', 'quickly', 'jump', 'dog', 'like', 'run']\n",
            "--------------------------------------------------\n",
            "--- 3. TF-IDF CALCULATION ---\n",
            "\n",
            "[CORPUS USED FOR TF-IDF]:\n",
            "  Document 1: The quick brown fox jumps over the lazy dog.\n",
            "  Document 2: A dog is lazy and a fox is quick.\n",
            "  Document 3: Quick animals like foxes and dogs are often found in nature.\n",
            "\n",
            "[TF-IDF MATRIX (Term Frequency-Inverse Document Frequency)]:\n",
            "Scores indicate term importance relative to the entire corpus.\n",
            "            animals   brown     dog    dogs     fox   foxes   jumps    lazy  \\\n",
            "Document 1   0.0000  0.4948  0.3763  0.0000  0.3763  0.0000  0.4948  0.3763   \n",
            "Document 2   0.0000  0.0000  0.5268  0.0000  0.5268  0.0000  0.0000  0.5268   \n",
            "Document 3   0.4324  0.0000  0.0000  0.4324  0.0000  0.4324  0.0000  0.0000   \n",
            "\n",
            "              like  nature   quick  \n",
            "Document 1  0.0000  0.0000  0.2923  \n",
            "Document 2  0.0000  0.0000  0.4091  \n",
            "Document 3  0.4324  0.4324  0.2554  \n",
            "\n",
            "[INTERPRETATION EXAMPLE]:\n",
            "The word 'quick' has a high score in Document 1 and 2 but a lower score in Document 3,\n",
            "meaning it is slightly less unique to Document 3 compared to the other two.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}